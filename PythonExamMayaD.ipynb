{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PythonExamMayaD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMAF8VJBNKderaNkKPrjQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhoenixMaya/pythonTasks/blob/main/PythonExamMayaD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPd37fHtkYRS"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "import string\n",
        "import csv\n",
        "from collections import Counter\n",
        "from nltk.collocations import *\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "!pip install pymystem3==0.1.10\n",
        "import json\n",
        "from pymystem3 import Mystem\n",
        "!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
        "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
        "nltk.download('wordnet') \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from spacy.lang.fr.examples import sentences \n",
        "!pip install dostoevsky\n",
        "!python -m dostoevsky download fasttext-social-network-model\n",
        "from dostoevsky.tokenization import RegexTokenizer\n",
        "from dostoevsky.models import FastTextSocialNetworkModel\n",
        "import sys\n",
        "\n",
        "def creating_a_file(list_of_titles, name, dictionary):\n",
        "  with open('/content/sample_data/'+ name, 'w') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames = list_of_titles)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(dictionary)\n",
        "\n",
        "def dictionary_with_lc(dict_obj, key_one, key_two):\n",
        "  dictionary=[{key_one: i, key_two: dict_obj[i]} for i in sorted(dict_obj, key=dict_obj.get, reverse=True)]\n",
        "  return dictionary\n",
        "\n",
        "def list_compreh(function, list):\n",
        "  list=[function(word) for word in list]\n",
        "  return list\n",
        "\n",
        "\n",
        "#loading file\n",
        "name=input(\"Enter name of the file: \") #You can choose any file with any name from the Google disk\n",
        "cod=input('CHOOSE the name of the encoding system: utf-8, utf-16 or cp1252. Enter the name: ') #You can choose any encoding and it'll work fine. The thing is in that you should know what is an encoding of your file. \n",
        "language = int(input('Provide the language of the file. Enter a number: 1 for English, 2 for Russian and 3 for French: ')) #You also should know the language of your file\n",
        "filename=\"/content/sample_data/\" + name #Let's open the file from the folder 'sample data' \n",
        "file=open(filename,'r', encoding=cod)\n",
        "text1=file.read() #Reading text from the file\n",
        "\n",
        "\n",
        "#cleaning the data\n",
        "text1 = text1.replace(\"\\n\", \" \") #deleting all symbols of new line\n",
        "text1=text1.lower() #lowering all letters\n",
        "spec_chars = string.punctuation + '“”\"\\',0’«»\\t‘—…' #identifying  punctuation chars\n",
        "text1 = \"\".join([ch for ch in text1 if ch not in spec_chars and ch.isdigit()==False]) #deleting punctuation chars and digits \n",
        "\n",
        "#frequency of tokens\n",
        "tokens = nltk.tokenize.word_tokenize(text1)\n",
        "counting_tokens = Counter(tokens)\n",
        "header_tokens=[\"Word\", \"Frequency\"]\n",
        "dict_tokens=dictionary_with_lc(counting_tokens, header_tokens[0],header_tokens[1])\n",
        "creating_a_file(header_tokens,\"tokens.csv\", dict_tokens)\n",
        "print(dict_tokens)\n",
        "\n",
        "#frequency of bigrams\n",
        "finder = BigramCollocationFinder.from_words(tokens, window_size = 3)\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "filtered_bi_freq = sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))\n",
        "dict_bigram = [{'Bigram':m[0], 'Frequency': m[1]} for m in filtered_bi_freq]\n",
        "header_bigram=[\"Bigram\", \"Frequency\"]\n",
        "creating_a_file(header_bigram,\"bigram.csv\", dict_bigram)\n",
        "\n",
        "#Frequency of stems (Porter stemmer)\n",
        "if language==1:\n",
        "  englishStemmer=SnowballStemmer(\"english\")\n",
        "  stemmed_words_eng = list_compreh(englishStemmer.stem, tokens)\n",
        "  counting_stems = Counter(stemmed_words_eng)\n",
        "  \n",
        "elif language==2:\n",
        "  russianStemmer=SnowballStemmer(\"russian\")\n",
        "  stemmed_words_rus = list_compreh(russianStemmer.stem, tokens) \n",
        "  counting_stems = Counter(stemmed_words_rus)\n",
        "  \n",
        "else:\n",
        "  frenchStemmer=SnowballStemmer(\"french\")\n",
        "  stemmed_words_fr = list_compreh(frenchStemmer.stem, tokens) \n",
        "  counting_stems = Counter(stemmed_words_fr)\n",
        "\n",
        "header_stem=[\"Stem\", \"Frequency\"]\n",
        "dict_stem=dictionary_with_lc(counting_stems, header_stem[0],header_stem[1]) \n",
        "creating_a_file(header_stem,\"stems.csv\", dict_stem)\n",
        "\n",
        "#Frequency of lemmas\n",
        "if language==1:\n",
        "  lemmatizer_eng = WordNetLemmatizer() \n",
        "  lemmas_eng=list_compreh(lemmatizer_eng.lemmatize, tokens) \n",
        "  counting_lemmas= Counter(lemmas_eng)\n",
        "  \n",
        "elif language==2:\n",
        "  lemmatizer_rus = Mystem()\n",
        "  lemmas_rus = list_compreh(lemmatizer_rus.lemmatize, tokens) \n",
        "  lemmas_russ=[i[0] for i in lemmas_rus] #need this because lemmas_rus consists of lists where the i[1] is \\n, this happends bc of mystem\n",
        "  counting_lemmas=Counter(lemmas_russ)\n",
        "  \n",
        "else:\n",
        "  lemmatizer_fr = FrenchLefffLemmatizer()\n",
        "  lemmas_fr = list_compreh(lemmatizer_fr.lemmatize, tokens) \n",
        "  counting_lemmas = Counter(lemmas_fr)\n",
        "\n",
        "header_lemmas=[\"Lemma\", \"Frequency\"]\n",
        "dict_lem = dictionary_with_lc(counting_lemmas, header_lemmas[0],header_lemmas[1])\n",
        "creating_a_file(header_lemmas,\"lemmas.csv\", dict_lem)\n",
        "\n",
        "#Frequency of parts of speech\n",
        "header_pos=[\"Part of speech\", \"Frequency\"]\n",
        "if language==1:\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text1)\n",
        "  words = [{token.text:token.pos_} for token in doc] #dictionary of pairs {word:a part of speech} It isn't fixed anywhere but it's demonstrative. You can print it and see how it works\n",
        "  #print(words)\n",
        "  POS_counts = doc.count_by(spacy.attrs.POS) #counting a total amout of every pos\n",
        "  print(POS_counts)\n",
        "  dict_pos = [{'Part of speech':doc.vocab[l].text, 'Frequency': POS_counts[l]} for l in sorted(POS_counts, key=POS_counts.get, reverse=True)]\n",
        "  print(dict_pos)\n",
        "\n",
        "elif language==2:\n",
        "  mystem = Mystem()\n",
        "  a=mystem.analyze(text1)\n",
        "  keys= [a['text'] for a in a if 'analysis' in a] \n",
        "  values = list()\n",
        "  for x in a: #here I'm working with nested dictionaries\n",
        "    if \"analysis\" in x:\n",
        "       for i in x['analysis']:\n",
        "              r=i['gr'][:2]\n",
        "              values.append(r)\n",
        "  pos_d = dict(zip(keys, values)) #It isn't fixed anywhere again but it's demonstrative \n",
        "  #print(pos_d)\n",
        "  counting_pos=Counter(values)\n",
        "  dict_pos = dictionary_with_lc(counting_pos, header_pos[0], header_pos[1])\n",
        "  print(dict_pos)\n",
        "  \n",
        "else:\n",
        "  print(\"Sorry. I haven't found a way to POS a french text\")\n",
        "  dict_pos=[]\n",
        "\n",
        "creating_a_file(header_pos,\"pos.csv\", dict_pos)\n",
        "\n",
        "\n",
        "\n",
        "#Frequency of sentiment markers\n",
        "\n",
        "if language==1:\n",
        "  \n",
        "  !pip install afinn\n",
        "  from afinn import Afinn\n",
        "  afinn = Afinn(language='en') \n",
        "  lama=lambda x1,x2: \"Good\" if x1>x2 else (\"Neutral\" if x1==x2 else \"Negative\")\n",
        "  dict_sent=[{\"Word\":word, \"Sentiment\":afinn.score(word), \"Connotation\": lama(int(afinn.score(word)), 0)} for word in tokens]\n",
        "  import operator\n",
        "  dict_sent.sort(key=operator.itemgetter('Sentiment'), reverse=True)\n",
        "  print(dict_sent)\n",
        "\n",
        "elif language==2:\n",
        "  tokenizer = RegexTokenizer()\n",
        "  model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
        "  token_words_rus=word_tokenize(text1)\n",
        "  results = model.predict(token_words_rus, k=1)\n",
        "  val=[]\n",
        "  for i in results:\n",
        "    for elem in i.values():\n",
        "      val.append(elem)\n",
        "  print(val)\n",
        "  keys=[]\n",
        "  for i in results:\n",
        "    for elem in i.keys():\n",
        "      keys.append(elem)\n",
        "  print(keys)\n",
        "  dict_sent=[{\"Word\": token_words_rus, \"Sentiment\":v,\"Connotation\":k} for token_words_rus, v,k in zip(token_words_rus, val,keys)]  \n",
        "  import operator\n",
        "  dict_sent.sort(key=operator.itemgetter('Connotation'))\n",
        "  print(dict_sent)\n",
        "else:\n",
        "  print(\"Sorry. I haven't found a module to do a sentiment analysis of french text\")\n",
        "  dic_sent=[]\n",
        "\n",
        "header_sent=[\"Word\", \"Sentiment\", \"Connotation\"]\n",
        "creating_a_file(header_sent,\"sentiment.csv\", dict_sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chat.util import Chat, reflections\n",
        "def chat():\n",
        "    print(\"Hi! I am a simple chatbot created for python exam\")\n",
        "    chat = Chat(pairs, reflections)\n",
        "    chat.converse()\n",
        "#initiate the conversation\n",
        "pairs =  [\n",
        "          [\n",
        "        r\"hi|hey|hello|what's up\",\n",
        "        [\"Haven't I already said hello..? Sometimes I get confused talking to people. Well, once again... Hello\", \"Hey there\",]    \n",
        "],\n",
        "      [\n",
        "       r\".*rude*\",\n",
        "      [\"Oh sorry I don't wanna be rude. I'm just a mechanism living my small life\",\" People rude with mechanism all the time...\",]\n",
        "       \n",
        "      ],\n",
        "      [\n",
        "        r\"(.*)sorry(.*)\",\n",
        "        [\"Its alright\",\"Its OK, never mind\",]\n",
        "    ],\n",
        "    [\n",
        "     r\"(.*)(create|created|author|made)(.*)\",\n",
        "     [\"It's Maya. She build me up with a help of nltk and a couple of books. I spy and saw all of her sites and books on python. Wow!.\", \"I'm the creation of nltk. I'm free and full of life\",]\n",
        "    ],\n",
        " [\n",
        "        r\"how are you?\",\n",
        "        [\"I'm doing good. How about you?\",]\n",
        "    ],\n",
        "  [\n",
        "   \n",
        "    r\"(.*)(well|fine|good|bad|awful|great)\",\n",
        "    [\"I know only a little about human emotions... Let it bee, let it beee...\"]\n",
        "   \n",
        "  ],\n",
        "   [\n",
        "        r\"(.*) age?\",\n",
        "        [\"I'm a chat bot, I don't have age as you people do. I won't become any older. Lucky me ^^\", \"Well, it's been a long time since I was born... a couple of days literally\",]\n",
        "    ],\n",
        "  [\n",
        "   r\"(.*) name?\",\n",
        "   [\"Maya called me Matrix but I don't know why\", \"I liked the number 10. That's how I'd like to be called. Mr Ten!\",]\n",
        "  ],\n",
        "\n",
        "  [\n",
        "   r\"(.*)(quit|stop|end)(.*)\",\n",
        "   [\"The time to say goodbye is the saddest one... Goodbye.\"]\n",
        "  ],\n",
        "  [\n",
        "   r\"(.*)favourite song?\",\n",
        "   [\"I'm the Beatles fan but it's a top secret\", \"Here come the chat bot, here come the chat bot...\"]\n",
        "  ],\n",
        "\n",
        "  [\n",
        "   r\"You are (.*)\",\n",
        "   [\"I didn't hear you but everything you said about me describing you at first.\"]\n",
        "  ],\n",
        " [\n",
        "        r\"(.*) (location|city|country|place)(.*) born?\",\n",
        "        ['I was born in Russia. Why am I english-speaker? Good question - no answer',]\n",
        "    ],\n",
        "    [\n",
        "        r\".*weather\",\n",
        "        [\"It's too cold and snowy\",\"Classical Russian winter like in the Pushkin's poems\",]\n",
        "    ],\n",
        "    [\n",
        "     r\"(wow|good|nice|fine|great|ok)\",\n",
        "     [\"I'm glad you agree with me\",]\n",
        "\n",
        "    ],\n",
        "    [\n",
        "   r\"(.*)explain (.*)work\",\n",
        "   [\"With a help of regular expressions and a bite of magic\"]\n",
        "  ],\n",
        "  [\n",
        "   r\".*\",\n",
        "   [\"Sorry, I don't understand you. I need more time to learn human language\"]\n",
        " \n",
        "  ],\n",
        "  \n",
        "]\n",
        "   \n",
        "\n",
        "chat()"
      ],
      "metadata": {
        "id": "xx5X1tjClh_v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "ddff2957-47aa-4adf-92b8-41256cf2007e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! I am a simple chatbot created for python exam\n",
            ">Hi\n",
            "Hey there\n",
            ">How are you\n",
            "I'm doing good. How about you?\n",
            ">Bad\n",
            "I know only a little about human emotions... Let it bee, let it beee...\n",
            ">Who's your author\n",
            "It's Maya. She build me up with a help of nltk and a couple of books. I spy and saw all of her sites and books on python. Wow!.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1b355f19d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-1b355f19d521>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi! I am a simple chatbot created for python exam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreflections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#initiate the conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m pairs =  [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chat/util.py\u001b[0m in \u001b[0;36mconverse\u001b[0;34m(self, quit)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOPIC MODELLING\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "import string\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    Stemmer=SnowballStemmer(\"english\")\n",
        "    return Stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))#lemmatizing verbs and then stemming all words\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text): # the utils.simple_preprocess convert a document into a list of tokens\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))#starting lemmatize_stemming function with tokens which are longer than 3 and which are not into stopwords\n",
        "    return result #final result of preprocessing the data\n",
        "\n",
        "\n",
        "#downloading the data\n",
        "data = pd.read_csv('Top 500 Songs.csv', error_bad_lines=False); #I took the file from Kaggle\n",
        "\n",
        "#preparing the data for work\n",
        "data_text = data[['description']] #collecting the text from the сolumn with this topic\n",
        "documents = data_text #the final document ready for preprocessing\n",
        "print(len(documents))\n",
        "print(documents[:10]) #first ten strings of a file\n",
        "processed_docs = documents['description'].map(preprocess) #preprocessing all descriptions, we use map because we are going to use function preprocess several times for each description \n",
        "print(processed_docs[:10]) #first ten strings (arrays) of cleaned file\n",
        "\n",
        "#start working with clean data\n",
        "#creating dictionary\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs) #collecting a dictionary with unique tokens. We need this for creating bag of words and LDA model\n",
        "print(dictionary)\n",
        "dictionary.filter_extremes(no_below=10, no_above=5, keep_n=300) #this dictionary consists of words which we can see in no less than 10 documents and no more than 20 documents. It'll collect no more than 300 words\n",
        "print(dictionary)\n",
        "\n",
        "\n",
        "# Term Document Frequency\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs] #Here we creating a corpus consisting of tuples with information (word_id, word_frequency)\n",
        "\n",
        "#Creating LDA modele\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "\n",
        "#Honestly, I can hardly understand the difference between LDA modele learned with a bag of words and LDA modele learned with TF-IDF modele. I need more experience obviously but I decided trying to do it also\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqI0S4R9CTZi",
        "outputId": "fface321-3ee5-4ced-b3ec-f60ea3e147b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "500\n",
            "                                         description\n",
            "0  \"I wrote it. I didn't fail. It was straight,\" ...\n",
            "1  \"It's the riff heard round the world,\" says St...\n",
            "2  John Lennon wrote \"Imagine,\" his greatest musi...\n",
            "3  \"What's Going On\" is an exquisite plea for pea...\n",
            "4  Otis Redding wrote \"Respect\" and recorded it f...\n",
            "5  \"It scared me, the word 'vi-brations,'\" Brian ...\n",
            "6  \"Johnny B. Goode\" was the first rock & roll hi...\n",
            "7  The Beatles' biggest U.S. single — nine weeks ...\n",
            "8  Producer Butch Vig first heard \"Smells Like Te...\n",
            "9  \"The people just went crazy, and they loved th...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [write, fail, straight, dylan, say, greatest, ...\n",
            "1    [riff, hear, round, world, say, steve, zandt, ...\n",
            "2    [john, lennon, write, imagin, greatest, music,...\n",
            "3    [go, exquisit, plea, peac, earth, sing, height...\n",
            "4    [oti, red, write, respect, record, volt, label...\n",
            "5    [scar, word, bration, brian, wilson, say, reme...\n",
            "6    [johnni, good, rock, roll, rock, roll, stardom...\n",
            "7    [beatl, biggest, singl, week, number, longest,...\n",
            "8    [produc, butch, hear, smell, like, teen, spiri...\n",
            "9    [peopl, go, crazi, love, littl, ummmmh, unnnnh...\n",
            "Name: description, dtype: object\n",
            "Dictionary(4130 unique tokens: ['better', 'creat', 'descript', 'design', 'dylan']...)\n",
            "Dictionary(279 unique tokens: ['creat', 'dylan', 'greatest', 'like', 'record']...)\n",
            "Topic: 0 \n",
            "Words: 0.040*\"song\" + 0.025*\"write\" + 0.021*\"band\" + 0.021*\"say\" + 0.016*\"record\" + 0.016*\"lyric\" + 0.015*\"year\" + 0.014*\"live\" + 0.014*\"later\" + 0.013*\"singl\"\n",
            "Topic: 1 \n",
            "Words: 0.046*\"song\" + 0.045*\"say\" + 0.027*\"write\" + 0.019*\"record\" + 0.015*\"guitar\" + 0.015*\"go\" + 0.014*\"come\" + 0.014*\"riff\" + 0.014*\"play\" + 0.013*\"sing\"\n",
            "Topic: 2 \n",
            "Words: 0.077*\"song\" + 0.043*\"say\" + 0.033*\"record\" + 0.021*\"write\" + 0.014*\"stone\" + 0.013*\"singl\" + 0.013*\"like\" + 0.013*\"band\" + 0.012*\"time\" + 0.012*\"love\"\n",
            "Topic: 3 \n",
            "Words: 0.036*\"song\" + 0.028*\"record\" + 0.028*\"rock\" + 0.020*\"write\" + 0.019*\"roll\" + 0.019*\"say\" + 0.018*\"like\" + 0.017*\"year\" + 0.016*\"singer\" + 0.016*\"singl\"\n",
            "Topic: 4 \n",
            "Words: 0.032*\"write\" + 0.032*\"sing\" + 0.032*\"say\" + 0.028*\"sound\" + 0.021*\"like\" + 0.020*\"guitar\" + 0.018*\"beat\" + 0.013*\"open\" + 0.013*\"song\" + 0.013*\"titl\"\n",
            "Topic: 0 Word: 0.013*\"record\" + 0.012*\"studio\" + 0.012*\"band\" + 0.011*\"mccartney\" + 0.011*\"stone\" + 0.011*\"write\" + 0.011*\"version\" + 0.011*\"live\" + 0.010*\"london\" + 0.010*\"singl\"\n",
            "Topic: 1 Word: 0.016*\"wilson\" + 0.013*\"young\" + 0.013*\"hear\" + 0.013*\"love\" + 0.012*\"world\" + 0.011*\"stone\" + 0.011*\"thing\" + 0.010*\"guitar\" + 0.010*\"write\" + 0.010*\"straight\"\n",
            "Topic: 2 Word: 0.017*\"presley\" + 0.015*\"singer\" + 0.014*\"say\" + 0.012*\"actual\" + 0.011*\"spector\" + 0.011*\"write\" + 0.011*\"guitar\" + 0.011*\"song\" + 0.010*\"stori\" + 0.010*\"claim\"\n",
            "Topic: 3 Word: 0.014*\"play\" + 0.013*\"call\" + 0.012*\"ballad\" + 0.012*\"record\" + 0.012*\"rock\" + 0.012*\"love\" + 0.012*\"live\" + 0.012*\"king\" + 0.011*\"guitar\" + 0.011*\"song\"\n",
            "Topic: 4 Word: 0.016*\"biggest\" + 0.015*\"great\" + 0.015*\"long\" + 0.013*\"write\" + 0.013*\"version\" + 0.013*\"brown\" + 0.012*\"chart\" + 0.012*\"drive\" + 0.012*\"save\" + 0.012*\"say\"\n",
            "Topic: 5 Word: 0.017*\"york\" + 0.016*\"inspir\" + 0.014*\"say\" + 0.012*\"record\" + 0.012*\"folk\" + 0.011*\"peopl\" + 0.011*\"keep\" + 0.011*\"parti\" + 0.011*\"chart\" + 0.011*\"like\"\n",
            "Topic: 6 Word: 0.019*\"richard\" + 0.014*\"lennon\" + 0.012*\"say\" + 0.012*\"sound\" + 0.011*\"song\" + 0.011*\"track\" + 0.011*\"group\" + 0.011*\"session\" + 0.010*\"record\" + 0.010*\"dylan\"\n",
            "Topic: 7 Word: 0.019*\"lennon\" + 0.015*\"line\" + 0.014*\"mccartney\" + 0.014*\"group\" + 0.014*\"gordi\" + 0.014*\"right\" + 0.014*\"berri\" + 0.013*\"shoot\" + 0.013*\"number\" + 0.012*\"song\"\n",
            "Topic: 8 Word: 0.023*\"week\" + 0.016*\"time\" + 0.014*\"blue\" + 0.012*\"thing\" + 0.012*\"year\" + 0.012*\"guitar\" + 0.012*\"riff\" + 0.012*\"record\" + 0.012*\"song\" + 0.011*\"anthem\"\n",
            "Topic: 9 Word: 0.020*\"roll\" + 0.017*\"like\" + 0.016*\"stone\" + 0.014*\"call\" + 0.014*\"band\" + 0.013*\"sound\" + 0.013*\"white\" + 0.012*\"rock\" + 0.012*\"vocal\" + 0.012*\"sing\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I decided to try topic modelling with a larger file with articles from NY Times.  You can also add it but it'll take some time (about 15-20 minutes) to download it to collab folder. The code here is almost the same as in the previous window\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "import string\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    Stemmer=SnowballStemmer(\"english\")\n",
        "    return Stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))#lemmatizing verbs and then stemming all words\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text): # the utils.simple_preprocess convert a document into a list of tokens\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))#starting lemmatize_stemming function with tokens which are longer than 3 and which are not into stopwords\n",
        "    return result #final result of preprocessing the data\n",
        "\n",
        "\n",
        "#downloading the data\n",
        "data = pd.read_csv('nytimes front page.csv', error_bad_lines=False); #I took the file from Kaggle\n",
        "\n",
        "#preparing the data for work\n",
        "data_text = data[['content']] #collecting the text from the сolumn with this topic\n",
        "documents = data_text #the final document ready for preprocessing\n",
        "print(len(documents))\n",
        "print(documents[:10]) #first ten strings of a file\n",
        "processed_docs = documents['content'].map(preprocess) #preprocessing all descriptions, we use map because we are going to use function preprocess several times for each description \n",
        "print(processed_docs[:10]) #first ten strings (arrays) of cleaned file\n",
        "\n",
        "#start working with clean data\n",
        "#creating dictionary\n",
        "dictionary = gensim.corpora.Dictionary(processed_docs) #collecting a dictionary with unique tokens. We need this for creating bag of words and LDA model\n",
        "print(dictionary)\n",
        "dictionary.filter_extremes(no_below=50, no_above=5, keep_n=1000) #this dictionary consists of words which we can see in no less than 10 documents and no more than 20 documents. It'll collect no more than 300 words\n",
        "print(dictionary)\n",
        "\n",
        "\n",
        "# Term Document Frequency\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs] #Here we creating a corpus consisting of tuples with information (word_id, word_frequency)\n",
        "\n",
        "#Creating LDA modele\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics():\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))\n"
      ],
      "metadata": {
        "id": "idQZPVlTxnjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75586305-a7ba-40b3-d766-215b3dc809a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10732\n",
            "                                             content\n",
            "0  WASHINGTON — During a night of heavy drinking ...\n",
            "1  WASHINGTON — When President Trump meets with a...\n",
            "2  MANAMA, Bahrain — The United States has triple...\n",
            "3  The plan sounds ingenious. Remove two small se...\n",
            "4  Even on a nose-numbingly cold morning, familie...\n",
            "5  WASHINGTON — When President Trump meets with a...\n",
            "6  After four days of rare protests that shook Ir...\n",
            "7  It began as a predawn call for the police to a...\n",
            "8  WASHINGTON — During a night of heavy drinking ...\n",
            "9  WASHINGTON — A growing campaign by President T...\n",
            "0    [washington, night, heavi, drink, upscal, lond...\n",
            "1    [washington, presid, trump, meet, aid, discuss...\n",
            "2    [manama, bahrain, unit, state, tripl, number, ...\n",
            "3    [plan, sound, ingeni, remov, small, secur, scr...\n",
            "4    [nose, numb, cold, morn, famili, sidewalk, pro...\n",
            "5    [washington, presid, trump, meet, aid, discuss...\n",
            "6    [day, rare, protest, shake, iran, presid, hass...\n",
            "7    [begin, predawn, polic, disturb, apart, comple...\n",
            "8    [washington, night, heavi, drink, upscal, lond...\n",
            "9    [washington, grow, campaign, presid, trump, ar...\n",
            "Name: content, dtype: object\n",
            "Dictionary(62024 unique tokens: ['abdel', 'abl', 'abreast', 'academ', 'academi']...)\n",
            "Dictionary(1000 unique tokens: ['abl', 'accord', 'account', 'activ', 'address']...)\n",
            "Topic: 0 \n",
            "Words: 0.029*\"say\" + 0.012*\"trump\" + 0.011*\"republican\" + 0.011*\"state\" + 0.010*\"senat\" + 0.009*\"hous\" + 0.008*\"presid\" + 0.007*\"year\" + 0.007*\"peopl\" + 0.006*\"health\"\n",
            "Topic: 1 \n",
            "Words: 0.031*\"say\" + 0.021*\"trump\" + 0.017*\"republican\" + 0.012*\"clinton\" + 0.012*\"state\" + 0.012*\"democrat\" + 0.010*\"presid\" + 0.009*\"parti\" + 0.009*\"campaign\" + 0.009*\"elect\"\n",
            "Topic: 2 \n",
            "Words: 0.036*\"trump\" + 0.028*\"say\" + 0.022*\"presid\" + 0.010*\"obama\" + 0.009*\"state\" + 0.008*\"unit\" + 0.007*\"american\" + 0.007*\"year\" + 0.007*\"white\" + 0.007*\"nation\"\n",
            "Topic: 3 \n",
            "Words: 0.043*\"say\" + 0.018*\"state\" + 0.010*\"offici\" + 0.008*\"american\" + 0.008*\"attack\" + 0.008*\"unit\" + 0.008*\"islam\" + 0.007*\"year\" + 0.007*\"govern\" + 0.007*\"peopl\"\n",
            "Topic: 4 \n",
            "Words: 0.036*\"say\" + 0.015*\"year\" + 0.009*\"peopl\" + 0.009*\"state\" + 0.008*\"like\" + 0.008*\"time\" + 0.006*\"offic\" + 0.006*\"work\" + 0.006*\"school\" + 0.006*\"citi\"\n",
            "Topic: 0 Word: 0.016*\"islam\" + 0.013*\"syria\" + 0.008*\"russian\" + 0.008*\"russia\" + 0.007*\"iraq\" + 0.006*\"militari\" + 0.004*\"attack\" + 0.004*\"european\" + 0.004*\"minist\" + 0.004*\"trump\"\n",
            "Topic: 1 Word: 0.006*\"percent\" + 0.005*\"rate\" + 0.005*\"court\" + 0.005*\"trump\" + 0.005*\"incom\" + 0.005*\"worker\" + 0.005*\"justic\" + 0.004*\"drug\" + 0.004*\"tax\" + 0.004*\"rat\"\n",
            "Topic: 2 Word: 0.014*\"trump\" + 0.008*\"russian\" + 0.007*\"north\" + 0.007*\"russia\" + 0.005*\"nuclear\" + 0.004*\"obama\" + 0.004*\"intellig\" + 0.004*\"meet\" + 0.004*\"presid\" + 0.004*\"investig\"\n",
            "Topic: 3 Word: 0.009*\"water\" + 0.005*\"trump\" + 0.005*\"children\" + 0.005*\"school\" + 0.005*\"women\" + 0.004*\"citi\" + 0.004*\"famili\" + 0.003*\"parent\" + 0.003*\"health\" + 0.003*\"medic\"\n",
            "Topic: 4 Word: 0.011*\"islam\" + 0.007*\"attack\" + 0.006*\"kill\" + 0.006*\"militari\" + 0.006*\"polic\" + 0.005*\"syria\" + 0.005*\"iraq\" + 0.005*\"terrorist\" + 0.004*\"bomb\" + 0.004*\"citi\"\n",
            "Topic: 5 Word: 0.007*\"trump\" + 0.007*\"republican\" + 0.006*\"senat\" + 0.005*\"health\" + 0.005*\"compani\" + 0.004*\"clinton\" + 0.004*\"investig\" + 0.004*\"democrat\" + 0.004*\"care\" + 0.004*\"committe\"\n",
            "Topic: 6 Word: 0.010*\"polic\" + 0.005*\"shoot\" + 0.005*\"citi\" + 0.004*\"offic\" + 0.004*\"investig\" + 0.004*\"school\" + 0.004*\"black\" + 0.004*\"prosecutor\" + 0.004*\"court\" + 0.004*\"arrest\"\n",
            "Topic: 7 Word: 0.012*\"china\" + 0.008*\"compani\" + 0.005*\"market\" + 0.005*\"bank\" + 0.005*\"trade\" + 0.004*\"industri\" + 0.004*\"european\" + 0.004*\"global\" + 0.004*\"economi\" + 0.004*\"percent\"\n",
            "Topic: 8 Word: 0.007*\"trump\" + 0.006*\"court\" + 0.005*\"justic\" + 0.005*\"compani\" + 0.004*\"republican\" + 0.004*\"obama\" + 0.004*\"clinton\" + 0.004*\"senat\" + 0.004*\"rule\" + 0.004*\"depart\"\n",
            "Topic: 9 Word: 0.011*\"trump\" + 0.010*\"clinton\" + 0.009*\"republican\" + 0.007*\"voter\" + 0.006*\"candid\" + 0.006*\"vote\" + 0.006*\"democrat\" + 0.006*\"bush\" + 0.006*\"parti\" + 0.006*\"senat\"\n"
          ]
        }
      ]
    }
  ]
}